{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binhvd/Data-Analytics-3-Solutions/blob/main/10_Sentiment_Analysis_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBegnwVciX3q"
      },
      "source": [
        "## Setup the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUDYEREAydsp"
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense, SimpleRNN, Activation, Dropout, Conv1D\n",
        "from tensorflow.keras.layers import Embedding, Flatten, LSTM, GRU\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Colab bug: https://github.com/googlecolab/colabtools/issues/3409\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda do_setlocale: \"UTF-8\""
      ],
      "metadata": {
        "id": "CgIWpmovOH3X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2GGZkNBiUDO"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFAlVSfR-LOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2d7f2b61-0e7f-4ab4-95d6-0327ff7a93a1"
      },
      "source": [
        "data = pd.read_csv(\"https://storage.googleapis.com/srh-dataset/sentiment-analysis/tweeter.csv\", header=None, encoding='latin-1')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0           1                             2         3                4  \\\n",
              "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
              "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
              "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
              "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
              "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
              "\n",
              "                                                   5  \n",
              "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
              "1  is upset that he can't update his Facebook by ...  \n",
              "2  @Kenichan I dived many times for the ball. Man...  \n",
              "3    my whole body feels itchy and like its on fire   \n",
              "4  @nationwideclass no, it's not behaving at all....  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e5cdb8d-19d7-41e1-9c86-8ecccd9cda5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e5cdb8d-19d7-41e1-9c86-8ecccd9cda5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8e5cdb8d-19d7-41e1-9c86-8ecccd9cda5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8e5cdb8d-19d7-41e1-9c86-8ecccd9cda5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TB_zc38FjQjE",
        "outputId": "bf114a0f-bf9e-42f9-eb8e-9db956429feb"
      },
      "source": [
        "# Check for missing values\n",
        "data.isnull().any()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    False\n",
              "1    False\n",
              "2    False\n",
              "3    False\n",
              "4    False\n",
              "5    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmH3mOVtjm35"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PBFIRBvqIY4s",
        "outputId": "153b1926-df09-415b-886e-ace18b4b50a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.10/dist-packages (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch) (2.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfhlThm6d-Z3"
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "from nltk.stem import SnowballStemmer \n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_stopwords_and_stemming(text, stem):\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "      if token not in stopwords:\n",
        "        # chops off the ends of words\n",
        "        if stem:\n",
        "          tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "          tokens.append(token)\n",
        "    return \" \".join(tokens)   \n",
        "\n",
        "def pre_process_corpus(docs, stem = False):\n",
        "    norm_docs = []\n",
        "    # tqdm to display a progess bar while looping\n",
        "    for doc in tqdm.tqdm(docs):\n",
        "        # remove HTML tags\n",
        "        doc = strip_html_tags(doc)\n",
        "\n",
        "        # convert tab, new lines to empty spaces    \n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "\n",
        "        # remove URL\n",
        "        doc = re.sub(r'http\\S+', '', doc)\n",
        "\n",
        "        # lowercase\n",
        "        doc = doc.lower()\n",
        "\n",
        "        # remove accented chars\n",
        "        doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "        # expand shortened words, e.g. don't to do not\n",
        "        doc = contractions.fix(doc)\n",
        "\n",
        "        # remove @username\n",
        "        doc = re.sub('@([A-Za-z0-9_]+)', ' ', doc)\n",
        "\n",
        "        # Replace all non alphabets.\n",
        "        doc = re.sub('[^a-zA-Z]', ' ', doc)\n",
        "\n",
        "        # Single character removal\n",
        "        #doc = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', doc)\n",
        "        \n",
        "        # remove white spaces\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()  \n",
        "\n",
        "        # remove stop words and apply stemming\n",
        "        doc = remove_stopwords_and_stemming(doc, stem)\n",
        "\n",
        "        norm_docs.append(doc)        \n",
        "    return norm_docs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_X = pre_process_corpus(data[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U2Z-W4HpIl_1",
        "outputId": "a5ca7d2d-9207-4b41-e15f-f3b98095d348"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20000 [00:00<?, ?it/s]<ipython-input-9-0d36f53ef83a>:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n",
            "100%|██████████| 20000/20000 [00:05<00:00, 3929.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EumLWGGMj-wW",
        "outputId": "2af4407e-7443-4d5f-be80-4d7882ad79c5"
      },
      "source": [
        "print(data[5][0])\n",
        "print(data_X[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "awww bummer shoulda got david carr third day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBS6ajOHkBBf"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_8gm-fukIQj"
      },
      "source": [
        "We only care about the tweet text and tweet sentiment information, which stored in the 5th column and 0th column in the dataset. In the sentiment column, 0 represents negative, and 1 represents positive.\n",
        "\n",
        "We organize the data as data_Xcontains all the tweet text, data_y contains the labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPV8cXIkyjR"
      },
      "source": [
        "The following code will convert the tweet text data_X to sequence format that will be feed into RNNs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UpIBFSi8oVyW",
        "outputId": "9279f971-59b2-4948-da03-1df51751e3d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bkOVxFEzkx-v",
        "outputId": "6fae2661-a180-4f86-b8a9-627b40122fb2"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load the twitter embeddings model. This model is trained on 2 billion tweets, which contains 27 billion tokens, 1.2 million vocabs.\n",
        "# might take a while\n",
        "glove_model = api.load(\"glove-twitter-200\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 758.4/758.5MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVX5M3jV_WOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d699c6ec-7925-4bc4-d713-f0b11a53da00"
      },
      "source": [
        "data_X = data[5]\n",
        "print(data_X)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1        is upset that he can't update his Facebook by ...\n",
            "2        @Kenichan I dived many times for the ball. Man...\n",
            "3          my whole body feels itchy and like its on fire \n",
            "4        @nationwideclass no, it's not behaving at all....\n",
            "                               ...                        \n",
            "19995    Just woke up. Having no school is the best fee...\n",
            "19996    TheWDB.com - Very cool to hear old Walt interv...\n",
            "19997    Are you ready for your MoJo Makeover? Ask me f...\n",
            "19998    Happy 38th Birthday to my boo of alll time!!! ...\n",
            "19999    happy #charitytuesday @theNSPCC @SparksCharity...\n",
            "Name: 5, Length: 20000, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt_DCR2SGvGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fddaa5a-c582-46aa-9c3a-b41f6ac8498b"
      },
      "source": [
        "data_y = pd.get_dummies(data[0]).to_numpy()\n",
        "print(data_y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0]\n",
            " [1 0]\n",
            " [1 0]\n",
            " ...\n",
            " [0 1]\n",
            " [0 1]\n",
            " [0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odrbjT-ClZEn"
      },
      "source": [
        "## Splitting Data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBWMDqltvnHK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, valid_X, train_y, valid_y = train_test_split(data_X, data_y, test_size = 0.2, random_state=42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "X3KasVGeR7-Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMpyUa8Dt8zH"
      },
      "source": [
        "max_vocab = 18000\n",
        "max_len = 15\n",
        "tokenizer = Tokenizer(num_words=max_vocab)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2mtdUG0uN8Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad0decf-d50e-4ad4-d006-2876ece5101f"
      },
      "source": [
        "tokenizer.fit_on_texts(train_X)\n",
        "\n",
        "train_X = tokenizer.texts_to_sequences(train_X)\n",
        "valid_X = tokenizer.texts_to_sequences(valid_X)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26000 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd230kpIvde3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8babbb-b176-43ea-ec49-bbb1f565d704"
      },
      "source": [
        "train_X = pad_sequences(train_X, maxlen=max_len, padding=\"post\")\n",
        "valid_X = pad_sequences(valid_X, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "train_X.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16000, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC90NhQYRzhQ",
        "outputId": "7c96e01c-40e3-4b0a-daf9-9f6e9b809997"
      },
      "source": [
        "train_X"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  47, 4062,   33, ...,  687, 2036,  337],\n",
              "       [   3,  197, 8118, ...,  780,  130,   36],\n",
              "       [8119,  126,  108, ...,  688,  108,   96],\n",
              "       ...,\n",
              "       [   5, 1052,  239, ...,    9,    0,    0],\n",
              "       [ 814,   31,   13, ...,    0,    0,    0],\n",
              "       [   6,    1,  827, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Gd1NpvNlfjI"
      },
      "source": [
        "### Preparing Word Embeddings using the GloVe Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_SIZE = 200"
      ],
      "metadata": {
        "id": "TNZHOhHUEFJl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6didFc14I-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44716e08-9f48-4faa-f552-347d3db53cdf"
      },
      "source": [
        "# calcultaete number of words\n",
        "nb_words = len(word_index) + 1\n",
        "print('All words: ', nb_words)\n",
        "\n",
        "# obtain the word embedding matrix\n",
        "embedding_matrix = np.zeros((nb_words, EMBED_SIZE))\n",
        "for word, i in word_index.items():\n",
        "    if word in glove_model:\n",
        "        embedding_matrix[i] = glove_model[word]\n",
        "\n",
        "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All words:  26001\n",
            "Null word embeddings: 10327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obYrO0MZTHN"
      },
      "source": [
        "**Explanation of the steps performed till now**\n",
        "\n",
        "Tweets: Is upset that he can't update his Facebook..\n",
        "\n",
        "Expected Input to RNN model - \n",
        "Is - Embeddings [200] (32)\n",
        "\n",
        "upset - Embeddings [200] (450)\n",
        "\n",
        "that - Embeddings [200] (43)\n",
        "\n",
        "he - Embeddings [200] (56)\n",
        "\n",
        "1. Vocabulary of all tweets: 30257 unique tokens\n",
        "2. Unique token IDs: ID (1, 2, 3, 4... for all the 30257 tokens)\n",
        "3. Tweets represented as the sequence of IDs [32 450 43 56 ...]\n",
        "\n",
        "Padding: \n",
        "\"Commonly in RNN's, we take the final output or hidden state and use this to make a prediction (or do whatever task we are trying to do).\n",
        "If we send a bunch of 0's to the RNN before taking the final output (i.e. 'post' padding as you describe), then the hidden state of the network at the final word in the sentence would likely get 'flushed out' to some extent by all the zero inputs that come after this word.\n",
        "So intuitively, this might be why pre-padding is more popular/effective.\" - [link](https://stackoverflow.com/questions/46298793/how-does-choosing-between-pre-and-post-zero-padding-of-sequences-impact-results)\n",
        "\n",
        "Padding for RNNs - [Link](https://datascience.stackexchange.com/questions/49168/padding-sequences-for-neural-sequence-models-rnns)\n",
        "\n",
        "[Paper](https://arxiv.org/abs/1903.07288)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgSQ0PA0lksS"
      },
      "source": [
        "### Build RNN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLfSN1Is43Kl"
      },
      "source": [
        "# adopted from sent_tran_eval.py\n",
        "def build_model(nb_words, rnn_model=\"SimpleRNN\", embedding_matrix=None):\n",
        "    '''\n",
        "    build_model function:\n",
        "    inputs: \n",
        "        rnn_model - which type of RNN layer to use, choose in (SimpleRNN, LSTM, GRU)\n",
        "        embedding_matrix - whether to use pretrained embeddings or not\n",
        "    '''\n",
        "    model = Sequential()\n",
        "    # add an embedding layer\n",
        "    if embedding_matrix is not None:\n",
        "        model.add(Embedding(nb_words, \n",
        "                        EMBED_SIZE, \n",
        "                        weights=[embedding_matrix], \n",
        "                        input_length= max_len,\n",
        "                        trainable = False))\n",
        "    else:\n",
        "        model.add(Embedding(nb_words, \n",
        "                        EMBED_SIZE, \n",
        "                        input_length= max_len,\n",
        "                        trainable = True))\n",
        "        \n",
        "    # add an RNN layer according to rnn_model\n",
        "    if rnn_model == \"SimpleRNN\":\n",
        "        model.add(SimpleRNN(EMBED_SIZE))\n",
        "    elif rnn_model == \"LSTM\":\n",
        "        model.add(LSTM(EMBED_SIZE))\n",
        "    else:\n",
        "        model.add(GRU(EMBED_SIZE))\n",
        "    # model.add(Dense(500,activation='relu'))\n",
        "    # model.add(Dense(500, activation='relu'))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A9N9LKhlwS_"
      },
      "source": [
        "### Training and Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEjtnTkLl1dG"
      },
      "source": [
        "Train and evaluate the SimpleRNN, LSTM, and GRU networks on our prepared dataset.\n",
        "\n",
        "We are using the pre-trained word embeddings from the glove.twitter.27B.200d.txt data. Using the pre-trained word embeddings as weights for the Embedding layer leads to better results and faster convergence.\n",
        "\n",
        "We set each models to run 20 epochs, but we also set EarlyStopping rules to prevent overfitting. The results of the SimpleRNN, LSTM, GRU models can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6vtAxYLSN50",
        "outputId": "80c74ce6-5911-421f-d26c-d7e2c33a3f03"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26001, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QoxYEfl6zr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a70d6ad-7ab9-4bbd-aae6-640e66fd7829"
      },
      "source": [
        "model_rnn = None\n",
        "model_rnn = build_model(nb_words, \"SimpleRNN\", embedding_matrix)\n",
        "model_rnn.fit(train_X, train_y, epochs=20, batch_size=120,\n",
        "          validation_data=(valid_X, valid_y), callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3))\n",
        "\n",
        "predictions = model_rnn.predict(valid_X)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(valid_y.argmax(axis=1), predictions))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "134/134 [==============================] - 10s 21ms/step - loss: 0.5941 - accuracy: 0.6824 - val_loss: 0.5700 - val_accuracy: 0.7067\n",
            "Epoch 2/20\n",
            "134/134 [==============================] - 3s 21ms/step - loss: 0.5132 - accuracy: 0.7501 - val_loss: 0.5264 - val_accuracy: 0.7510\n",
            "Epoch 3/20\n",
            "134/134 [==============================] - 4s 29ms/step - loss: 0.4829 - accuracy: 0.7705 - val_loss: 0.5169 - val_accuracy: 0.7485\n",
            "Epoch 4/20\n",
            "134/134 [==============================] - 3s 20ms/step - loss: 0.4589 - accuracy: 0.7837 - val_loss: 0.5336 - val_accuracy: 0.7377\n",
            "Epoch 5/20\n",
            "134/134 [==============================] - 3s 20ms/step - loss: 0.4315 - accuracy: 0.7979 - val_loss: 0.5657 - val_accuracy: 0.7200\n",
            "125/125 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.85      0.75      2018\n",
            "           1       0.79      0.59      0.67      1982\n",
            "\n",
            "    accuracy                           0.72      4000\n",
            "   macro avg       0.74      0.72      0.71      4000\n",
            "weighted avg       0.74      0.72      0.71      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xFS8uEnJNV"
      },
      "source": [
        "## In-Class Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq0tO71qnMHC"
      },
      "source": [
        "### Try training the RNNs without the pre-trained word embeddings and compare the results with the pre-trained model.\n",
        "\n",
        "1. Does word embeddings impact the accuracy?\n",
        "2. If we tweak the units in RNN does it impact the accuracy?\n",
        "3. How data pre-processing impact the accuracy? (Pre and post padding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlA_9skrnjNk"
      },
      "source": [
        "## LSTM and GRUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhj2fWHW-MZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d3287a-70d8-4d5f-a055-b7f38cf005d8"
      },
      "source": [
        "model_lstm = build_model(nb_words, \"LSTM\", embedding_matrix)\n",
        "model_lstm.fit(train_X, train_y, epochs=20, batch_size=120,\n",
        "          validation_data=(valid_X, valid_y), callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3))\n",
        "\n",
        "predictions = model_lstm.predict(valid_X)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(valid_y.argmax(axis=1), predictions))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "134/134 [==============================] - 6s 13ms/step - loss: 0.5575 - accuracy: 0.7076 - val_loss: 0.4938 - val_accuracy: 0.7595\n",
            "Epoch 2/20\n",
            "134/134 [==============================] - 1s 9ms/step - loss: 0.4966 - accuracy: 0.7578 - val_loss: 0.4860 - val_accuracy: 0.7617\n",
            "Epoch 3/20\n",
            "134/134 [==============================] - 1s 9ms/step - loss: 0.4706 - accuracy: 0.7733 - val_loss: 0.4800 - val_accuracy: 0.7690\n",
            "Epoch 4/20\n",
            "134/134 [==============================] - 1s 9ms/step - loss: 0.4536 - accuracy: 0.7836 - val_loss: 0.4760 - val_accuracy: 0.7690\n",
            "Epoch 5/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.4252 - accuracy: 0.8019 - val_loss: 0.4852 - val_accuracy: 0.7645\n",
            "Epoch 6/20\n",
            "134/134 [==============================] - 1s 9ms/step - loss: 0.3978 - accuracy: 0.8144 - val_loss: 0.5061 - val_accuracy: 0.7575\n",
            "125/125 [==============================] - 1s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.69      0.74      2018\n",
            "           1       0.73      0.82      0.77      1982\n",
            "\n",
            "    accuracy                           0.76      4000\n",
            "   macro avg       0.76      0.76      0.76      4000\n",
            "weighted avg       0.76      0.76      0.76      4000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8aoae6jN7tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077a8dda-ff35-439b-f6dd-f891e8f5d292"
      },
      "source": [
        "model_gru = build_model(nb_words, \"GRU\", embedding_matrix)\n",
        "model_gru.fit(train_X, train_y, epochs=20, batch_size=120,\n",
        "          validation_data=(valid_X, valid_y), callbacks=EarlyStopping(monitor='val_accuracy', mode='max',patience=3))\n",
        "\n",
        "predictions = model_gru.predict(valid_X)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(valid_y.argmax(axis=1), predictions))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "134/134 [==============================] - 5s 13ms/step - loss: 0.5734 - accuracy: 0.6888 - val_loss: 0.5079 - val_accuracy: 0.7427\n",
            "Epoch 2/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.4954 - accuracy: 0.7533 - val_loss: 0.4891 - val_accuracy: 0.7655\n",
            "Epoch 3/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.4715 - accuracy: 0.7731 - val_loss: 0.4873 - val_accuracy: 0.7623\n",
            "Epoch 4/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.4511 - accuracy: 0.7864 - val_loss: 0.4830 - val_accuracy: 0.7657\n",
            "Epoch 5/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.4263 - accuracy: 0.8023 - val_loss: 0.4791 - val_accuracy: 0.7670\n",
            "Epoch 6/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.3955 - accuracy: 0.8203 - val_loss: 0.4945 - val_accuracy: 0.7600\n",
            "Epoch 7/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.3580 - accuracy: 0.8422 - val_loss: 0.4984 - val_accuracy: 0.7623\n",
            "Epoch 8/20\n",
            "134/134 [==============================] - 1s 8ms/step - loss: 0.3115 - accuracy: 0.8659 - val_loss: 0.5257 - val_accuracy: 0.7570\n",
            "125/125 [==============================] - 1s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.76      0.76      2018\n",
            "           1       0.75      0.76      0.76      1982\n",
            "\n",
            "    accuracy                           0.76      4000\n",
            "   macro avg       0.76      0.76      0.76      4000\n",
            "weighted avg       0.76      0.76      0.76      4000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}